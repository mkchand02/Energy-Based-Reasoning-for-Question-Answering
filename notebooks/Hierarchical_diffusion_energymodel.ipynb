{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 14065190,
          "sourceType": "datasetVersion",
          "datasetId": 8952627
        }
      ],
      "dockerImageVersionId": 31193,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# HVAE EBM\n",
        "This notebook is designed to train a Hierarchical Variational Autoencoder (HVAE) combined with an Energy-Based Model (EBM) for natural language processing tasks. Here's a summary of the key steps:\n",
        "\n",
        "**Data Preparation**: We load precomputed dataset of questions and answers.\n",
        "and use a DistilBert model to encode the questions into variable-length embeddings.\n",
        "**Custom Data Loading**: A custom collate_fn is implemented to handle the variable-length question embeddings and create appropriate masks for batch processing.\n",
        "**Model Architecture**:\n",
        "\n",
        "Autoregressive HVAE_Transform: Manages the transformation between noise and hierarchical latent variables.\n",
        "\n",
        "Cross AttentionEBM and HierarchicalTransformerEBM: Implement the Energy-Based Model using cross-attention, designed to work with both global and local latent variables.\n",
        "\n",
        "HT_HVAE_InferenceNetwork (Encoder): Uses DistilBert and a Transformer to encode text into global and local latent distributions.\n",
        "\n",
        "HT_HVAE_GenerativeNetwork (Decoder): Employs a GRU for sentence planning and a modified GPT-2 for word generation, conditioned on the latent variables.\n",
        "\n",
        "Model Loading: Loads pre-trained weights for the HVAE encoder and decoder from a Weights & Biases artifact.\n",
        "\n",
        "EBM Training Setup: Initializes the HVAE transform and EBM models, sets up an optimizer, and defines a diffusion noise schedule for training the EBM."
      ],
      "metadata": {
        "id": "lzGQvW5dTJcA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and Data Loading"
      ],
      "metadata": {
        "id": "RGWcJzxGUBTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-09T02:30:17.735007Z",
          "iopub.execute_input": "2025-12-09T02:30:17.735177Z",
          "iopub.status.idle": "2025-12-09T02:30:19.902155Z",
          "shell.execute_reply.started": "2025-12-09T02:30:17.735161Z",
          "shell.execute_reply": "2025-12-09T02:30:19.901503Z"
        },
        "id": "qB3PKyGgS3pG",
        "outputId": "90bb43a9-d374-4132-cede-6b575071cccf"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "/kaggle/input/h-latents-dataset/hvae_dataset_with_latents.pt\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login(key=\"0ce56922c7ea30310a87d49246b15bc7d7ca9c89\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-09T03:39:52.498258Z",
          "iopub.execute_input": "2025-12-09T03:39:52.498816Z",
          "iopub.status.idle": "2025-12-09T03:40:01.541323Z",
          "shell.execute_reply.started": "2025-12-09T03:39:52.498792Z",
          "shell.execute_reply": "2025-12-09T03:40:01.540589Z"
        },
        "id": "I493traSS3pH",
        "outputId": "d0a6822c-e18d-4a1d-b007-41b2194aaf89"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myasir-alam14\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
          "output_type": "stream"
        },
        {
          "execution_count": 37,
          "output_type": "execute_result",
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from transformers import AutoTokenizer\n",
        "import numpy as np\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "import torch.nn.utils as utils\n",
        "from tqdm import tqdm\n",
        "from transformers import GPT2Model, GPT2Config\n",
        "from sklearn.model_selection import train_test_split\n",
        "from bs4 import BeautifulSoup\n",
        "from transformers import DistilBertModel, DistilBertConfig\n",
        "from transformers import DistilBertTokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import gc"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-09T02:30:19.903676Z",
          "iopub.execute_input": "2025-12-09T02:30:19.904313Z",
          "iopub.status.idle": "2025-12-09T02:31:12.235804Z",
          "shell.execute_reply.started": "2025-12-09T02:30:19.904294Z",
          "shell.execute_reply": "2025-12-09T02:31:12.235226Z"
        },
        "id": "YY1JZ4pLS3pI",
        "outputId": "8c42525c-29ad-46be-b804-601cfb1a1f9c"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "2025-12-09 02:30:43.390726: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1765247443.698298      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1765247443.788057      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ],
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "output_type": "error"
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ],
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "output_type": "error"
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ],
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "output_type": "error"
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ],
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "output_type": "error"
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ],
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class PrecomputedDataset(Dataset):\n",
        "    \"\"\"a custom PyTorch Dataset that loads precomputed data from a .pt file.\"\"\"\n",
        "    def __init__(self, pt_file_path):\n",
        "        self.data = torch.load(pt_file_path)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-09T02:31:12.236631Z",
          "iopub.execute_input": "2025-12-09T02:31:12.237212Z",
          "iopub.status.idle": "2025-12-09T02:31:12.241910Z",
          "shell.execute_reply.started": "2025-12-09T02:31:12.237161Z",
          "shell.execute_reply": "2025-12-09T02:31:12.241142Z"
        },
        "id": "eilXQk-2S3pI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = PrecomputedDataset('/kaggle/input/h-latents-dataset/hvae_dataset_with_latents.pt')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-09T02:31:12.242980Z",
          "iopub.execute_input": "2025-12-09T02:31:12.243326Z",
          "iopub.status.idle": "2025-12-09T02:31:20.035970Z",
          "shell.execute_reply.started": "2025-12-09T02:31:12.243293Z",
          "shell.execute_reply": "2025-12-09T02:31:20.035401Z"
        },
        "id": "wGpiXSztS3pJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataset)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-09T02:31:20.036701Z",
          "iopub.execute_input": "2025-12-09T02:31:20.036905Z",
          "iopub.status.idle": "2025-12-09T02:31:20.042924Z",
          "shell.execute_reply.started": "2025-12-09T02:31:20.036889Z",
          "shell.execute_reply": "2025-12-09T02:31:20.042212Z"
        },
        "id": "gqoGevPmS3pJ",
        "outputId": "fc768a5d-f8e5-4cdd-946c-f474d7261875"
      },
      "outputs": [
        {
          "execution_count": 5,
          "output_type": "execute_result",
          "data": {
            "text/plain": "15259"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch DataLoader that loads one item at a time for initial data inspection.\n",
        "train_loader = DataLoader(train_dataset, batch_size = 1)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-09T02:31:20.044587Z",
          "iopub.execute_input": "2025-12-09T02:31:20.044855Z",
          "iopub.status.idle": "2025-12-09T02:31:20.098502Z",
          "shell.execute_reply.started": "2025-12-09T02:31:20.044836Z",
          "shell.execute_reply": "2025-12-09T02:31:20.097962Z"
        },
        "id": "yw3CSOl-S3pJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initializes the DistilBertTokenizer and DistilBertModel from Hugging Face\n",
        "They are used for data encoding for the HVAE's inference network."
      ],
      "metadata": {
        "id": "DBDpik_0UiRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "model = DistilBertModel.from_pretrained('distilbert-base-uncased').to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-09T02:31:20.099078Z",
          "iopub.execute_input": "2025-12-09T02:31:20.099311Z",
          "iopub.status.idle": "2025-12-09T02:31:24.493643Z",
          "shell.execute_reply.started": "2025-12-09T02:31:20.099294Z",
          "shell.execute_reply": "2025-12-09T02:31:24.492870Z"
        },
        "colab": {
          "referenced_widgets": [
            "f522ce82faab48769c69510d28eba1e6",
            "d43193267abe4d1daf3b769384a5d33b",
            "4de94d97889a4f6994247ed264a64c06",
            "2c2d2f961e5248fca74f109cb584747e",
            "ebc3354c02234a8ab64cc5ab7b1d2e44"
          ]
        },
        "id": "HZMDJLxOS3pK",
        "outputId": "33a0dc9b-950a-4c0f-f663-e99847862333"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f522ce82faab48769c69510d28eba1e6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d43193267abe4d1daf3b769384a5d33b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4de94d97889a4f6994247ed264a64c06"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2c2d2f961e5248fca74f109cb584747e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ebc3354c02234a8ab64cc5ab7b1d2e44"
            }
          },
          "metadata": {}
        },
        {
          "execution_count": 7,
          "output_type": "execute_result",
          "data": {
            "text/plain": "DistilBertModel(\n  (embeddings): Embeddings(\n    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n    (position_embeddings): Embedding(512, 768)\n    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (transformer): Transformer(\n    (layer): ModuleList(\n      (0-5): 6 x TransformerBlock(\n        (attention): DistilBertSdpaAttention(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (ffn): FFN(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          (activation): GELUActivation()\n        )\n        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n    )\n  )\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in train_loader:\n",
        "    print(batch['question'])\n",
        "    break"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-09T02:31:33.453054Z",
          "iopub.execute_input": "2025-12-09T02:31:33.453350Z",
          "iopub.status.idle": "2025-12-09T02:31:33.469409Z",
          "shell.execute_reply.started": "2025-12-09T02:31:33.453327Z",
          "shell.execute_reply": "2025-12-09T02:31:33.468811Z"
        },
        "id": "HzcTkmK8S3pK",
        "outputId": "a133b23e-1a9d-470e-ebbe-c8fa61b6ea18"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "['I recently learnt the concept that electric field lines do not cut each other this statement was proved with the logic that :\\n\" If electric field lines would intersect then there would be ultimately two directions of tangents for that very point which contradicts the laws of electric field lines.\"\\n\\nBut I\\'m wondering if there\\'s only one line of tangent associated with the electric field lines then how it would not have two directions with it. Namely;\\nparallel and antiparallel directions']\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iterate through the train_loader, encoding each question using the loaded DistilBertModel (without padding to get variable-length embeddings). We also save the augmented data to a new .pt file named hvae_dataset_variable_len.pt. This pre-processes the question text into embeddings for efficiency."
      ],
      "metadata": {
        "id": "yoZXY34xU_eM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_data_list = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(train_loader, desc=\"Encoding Questions\"):\n",
        "\n",
        "        raw_question = batch['question'][0]\n",
        "\n",
        "        # CHANGE 1: padding=False.\n",
        "        # The tensor size will exactly match the word count.\n",
        "        inputs = tokenizer(\n",
        "            raw_question,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=False,  # <--- Crucial change\n",
        "            truncation=True,\n",
        "            max_length=512\n",
        "        ).to(device)\n",
        "\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        # Shape: [Real_Seq_Len, 768] (e.g., [15, 768])\n",
        "        full_embeddings = outputs.last_hidden_state[0].cpu()\n",
        "\n",
        "        new_item = {\n",
        "            'question': raw_question,\n",
        "            'answer': batch['answer'][0],\n",
        "            'enc_inputs': batch['enc_inputs'][0],\n",
        "            'enc_wordMask': batch['enc_wordMask'][0],\n",
        "            'dec_inputs': batch['dec_inputs'][0],\n",
        "            'dec_wordmask': batch['dec_wordmask'][0],\n",
        "            'global_latents': batch['global_latents'][0],\n",
        "            'local_latents': batch['local_latents'][0],\n",
        "            'question_encoded': full_embeddings\n",
        "        }\n",
        "\n",
        "        new_data_list.append(new_item)\n",
        "\n",
        "torch.save(new_data_list, 'hvae_dataset_variable_len.pt')\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-09T02:31:35.663624Z",
          "iopub.execute_input": "2025-12-09T02:31:35.664204Z",
          "iopub.status.idle": "2025-12-09T02:34:05.548088Z",
          "shell.execute_reply.started": "2025-12-09T02:31:35.664168Z",
          "shell.execute_reply": "2025-12-09T02:34:05.547435Z"
        },
        "id": "Hht08Px7S3pL",
        "outputId": "55c59718-d28e-4eba-ce57-e2be25443583"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "Encoding Questions: 100%|██████████| 15259/15259 [02:04<00:00, 122.34it/s]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class DistilBertAugmentedDataset(Dataset):\n",
        "    \"\"\"similar to PrecomputedDataset. Loads the newly created hvae_dataset_variable_len.pt file.\"\"\"\n",
        "    def __init__(self, path):\n",
        "        self.data = torch.load(path)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "new_dataset = DistilBertAugmentedDataset('/kaggle/working/hvae_dataset_variable_len.pt')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-09T02:50:23.212697Z",
          "iopub.execute_input": "2025-12-09T02:50:23.213385Z",
          "iopub.status.idle": "2025-12-09T02:50:58.585125Z",
          "shell.execute_reply.started": "2025-12-09T02:50:23.213362Z",
          "shell.execute_reply": "2025-12-09T02:50:58.578679Z"
        },
        "id": "toU7AMpZS3pM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "del new_data_list\n",
        "gc.collect()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-09T02:50:58.586130Z",
          "iopub.execute_input": "2025-12-09T02:50:58.586422Z",
          "iopub.status.idle": "2025-12-09T02:50:58.617105Z",
          "shell.execute_reply.started": "2025-12-09T02:50:58.586403Z",
          "shell.execute_reply": "2025-12-09T02:50:58.616233Z"
        },
        "id": "ONCXxLQ-S3pM",
        "outputId": "48dde599-9626-4441-d4d8-65ba982c2560"
      },
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_47/3976181758.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mnew_data_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'new_data_list' is not defined"
          ],
          "ename": "NameError",
          "evalue": "name 'new_data_list' is not defined",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "hvae_collate_fn, a custom collate function for the DataLoader. This function handles padding of variable-length question embeddings and creates corresponding masks when batching samples. It then re-creates new_train_loader using this custom function."
      ],
      "metadata": {
        "id": "GR_0tLPQVYrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def hvae_collate_fn(batch):\n",
        "    # Initialize lists\n",
        "    question_embs = []\n",
        "    question_masks = []  # We will generate these now\n",
        "\n",
        "    # Lists for other fixed columns\n",
        "    enc_inputs = []\n",
        "    enc_wordMask = []\n",
        "    dec_inputs = []\n",
        "    dec_wordmask = []\n",
        "    global_latents = []\n",
        "    local_latents = []\n",
        "    questions_text = []\n",
        "    answers_text = []\n",
        "\n",
        "    for item in batch:\n",
        "        # 1. Get the variable-length embedding\n",
        "        # Check both key names just in case\n",
        "        q_emb = item.get('question_emb', item.get('question_encoded'))\n",
        "\n",
        "        # 2. CREATE THE MASK ON THE FLY\n",
        "        # Since 'q_emb' contains only real data, the mask is all 1s (True)\n",
        "        seq_len = q_emb.shape[0]\n",
        "        q_mask = torch.ones(seq_len, dtype=torch.bool)\n",
        "\n",
        "        question_embs.append(q_emb)\n",
        "        question_masks.append(q_mask)\n",
        "\n",
        "        # Collect other items\n",
        "        enc_inputs.append(item['enc_inputs'])\n",
        "        enc_wordMask.append(item['enc_wordMask'])\n",
        "        dec_inputs.append(item['dec_inputs'])\n",
        "        dec_wordmask.append(item['dec_wordmask'])\n",
        "        global_latents.append(item['global_latents'])\n",
        "        local_latents.append(item['local_latents'])\n",
        "\n",
        "        questions_text.append(item['question'])\n",
        "        answers_text.append(item['answer'])\n",
        "\n",
        "    # 3. Pad Sequences\n",
        "    # Pad embeddings with 0.0\n",
        "    padded_q_emb = pad_sequence(question_embs, batch_first=True, padding_value=0.0)\n",
        "\n",
        "    # Pad masks with False (0).\n",
        "    # Result: 11111000 (1=Real, 0=Pad)\n",
        "    padded_q_mask = pad_sequence(question_masks, batch_first=True, padding_value=False)\n",
        "\n",
        "    return {\n",
        "        'question_encoded': padded_q_emb,\n",
        "        'question_mask': padded_q_mask, # Now this exists!\n",
        "\n",
        "        'enc_inputs': torch.stack(enc_inputs),\n",
        "        'enc_wordMask': torch.stack(enc_wordMask),\n",
        "        'dec_inputs': torch.stack(dec_inputs),\n",
        "        'dec_wordmask': torch.stack(dec_wordmask),\n",
        "        'global_latents': torch.stack(global_latents),\n",
        "        'local_latents': torch.stack(local_latents),\n",
        "\n",
        "        'question': questions_text,\n",
        "        'answer': answers_text\n",
        "    }\n",
        "\n",
        "# Re-create the loader\n",
        "new_train_loader = DataLoader(\n",
        "    new_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        "    collate_fn=hvae_collate_fn\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-09T02:56:17.481888Z",
          "iopub.execute_input": "2025-12-09T02:56:17.482167Z",
          "iopub.status.idle": "2025-12-09T02:56:17.490304Z",
          "shell.execute_reply.started": "2025-12-09T02:56:17.482146Z",
          "shell.execute_reply": "2025-12-09T02:56:17.489643Z"
        },
        "id": "BvgwWNOvS3pM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# We iterates through the new_train_loader once and prints the shapes and lengths\n",
        "# of various tensors and lists within a batch, to verify if the collate_fn is working\n",
        "# as expected and producing the correct tensor dimensions, including the padded question embeddings and masks.\n",
        "\n",
        "for batch in new_train_loader:\n",
        "    print(len(batch['question']))\n",
        "    print(len(batch['answer']))\n",
        "    print(batch['enc_inputs'].shape)\n",
        "    print(batch['enc_wordMask'].shape)\n",
        "    print(batch['dec_inputs'].shape)\n",
        "    print(batch['dec_wordmask'].shape)\n",
        "    print(batch['global_latents'].shape)\n",
        "    print(batch['local_latents'].shape)\n",
        "    print(batch['question_encoded'].shape)\n",
        "    print(batch['question_mask'].shape)\n",
        "    print(batch['global_latents'].shape)\n",
        "    print(batch['local_latents'].shape)\n",
        "    break"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-09T03:04:32.260258Z",
          "iopub.execute_input": "2025-12-09T03:04:32.260756Z",
          "iopub.status.idle": "2025-12-09T03:04:32.267788Z",
          "shell.execute_reply.started": "2025-12-09T03:04:32.260736Z",
          "shell.execute_reply": "2025-12-09T03:04:32.267216Z"
        },
        "id": "YhKcf3KxS3pM",
        "outputId": "b93508f0-10d0-43d2-fc5b-630bbfa5cf19"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "1\n1\ntorch.Size([1, 11, 50])\ntorch.Size([1, 11, 50])\ntorch.Size([1, 11, 50])\ntorch.Size([1, 11, 50])\ntorch.Size([1, 32])\ntorch.Size([1, 11, 32])\ntorch.Size([1, 94, 768])\ntorch.Size([1, 94])\ntorch.Size([1, 32])\ntorch.Size([1, 11, 32])\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Architecture"
      ],
      "metadata": {
        "id": "RNpZCBNGWxUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class AutoregressiveHVAE_Transform(nn.Module):\n",
        "    \"\"\"\n",
        "    class to handle the transformation between independent noise variables (u)\n",
        "    and hierarchical latent variables (z), including both global and local latents,\n",
        "    with an autoregressive dependency for local latents.\n",
        "    \"\"\"\n",
        "    def __init__(self, prior_net, latent_dim=32, seq_len=10):\n",
        "        super().__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        # We now use the sophisticated MLPNetworkForPrior you defined earlier\n",
        "        self.prior_net = prior_net\n",
        "\n",
        "    def u_to_z(self, u_list):\n",
        "        \"\"\"\n",
        "        Transform independent Noise (u) -> Hierarchical Latents (z)\n",
        "        Must be SEQUENTIAL (Loop) because z_i depends on z_{i-1}\n",
        "        \"\"\"\n",
        "        u_global, u_local_seq = u_list\n",
        "        # u_global: (B, D)\n",
        "        # u_local_seq: (B, Seq, D)\n",
        "\n",
        "        batch_size = u_global.shape[0]\n",
        "\n",
        "        # 1. Global Level (Standard Normal Prior)\n",
        "        # z_t = u_t (since prior is N(0,I))\n",
        "        z_t = u_global\n",
        "\n",
        "        # 2. Local Level (Autoregressive Loop)\n",
        "        z_local_list = []\n",
        "\n",
        "        # Initialize z_{i-1} (z_prev) as zero vector for the first step\n",
        "        z_prev = torch.zeros(batch_size, self.latent_dim, device=z_t.device)\n",
        "\n",
        "        # Loop through time steps\n",
        "        for i in range(self.seq_len):\n",
        "            # Get the noise for this specific step\n",
        "            u_i = u_local_seq[:, i, :]\n",
        "\n",
        "            # Predict Prior Params: p(z_i | z_t, z_{i-1})\n",
        "            mu_i, sigma2_i = self.prior_net(z_t, z_prev)\n",
        "            sigma_i = torch.sqrt(sigma2_i)\n",
        "\n",
        "            # Reparameterize: z_i = mu + sigma * u\n",
        "            z_i = mu_i + sigma_i * u_i\n",
        "\n",
        "            # Store and Update State\n",
        "            z_local_list.append(z_i)\n",
        "            z_prev = z_i\n",
        "\n",
        "        # Stack list into tensor (B, Seq, D)\n",
        "        z_local = torch.stack(z_local_list, dim=1)\n",
        "\n",
        "        return [z_t, z_local]\n",
        "\n",
        "    def z_to_u(self, z_list):\n",
        "        \"\"\"\n",
        "        Inverse Transform: Recover Noise (u) from Latents (z)\n",
        "        Can be PARALLELIZED using \"Teacher Forcing\" (Shifting inputs)\n",
        "        \"\"\"\n",
        "        z_t, z_local = z_list\n",
        "        # z_t: (B, D)\n",
        "        # z_local: (B, Seq, D)\n",
        "\n",
        "        batch_size = z_t.shape[0]\n",
        "        device = z_t.device\n",
        "\n",
        "        # 1. Global Level (z_t -> u_t)\n",
        "        u_t = z_t\n",
        "\n",
        "        # 2. Local Level (z_local -> u_local)\n",
        "        # To compute u_i, we need the mean/std predicted by the prior.\n",
        "        # The prior needs (z_t, z_{i-1}).\n",
        "\n",
        "        # Create z_{i-1} sequence by shifting z_local to the right\n",
        "        # [z1, z2, z3] -> [0, z1, z2]\n",
        "        zeros = torch.zeros(batch_size, 1, self.latent_dim, device=device)\n",
        "        z_prev_seq = torch.cat([zeros, z_local[:, :-1, :]], dim=1)\n",
        "\n",
        "        # Expand z_t to match sequence length for batch processing\n",
        "        # (B, D) -> (B, Seq, D)\n",
        "        z_t_expanded = z_t.unsqueeze(1).expand(-1, self.seq_len, -1)\n",
        "\n",
        "        # Run Prior Net in PARALLEL on the whole sequence\n",
        "        # Note: Your MLPNetworkForPrior uses Linear layers, so it handles (B, Seq, D) automatically\n",
        "        mu_seq, sigma2_seq = self.prior_net(z_t_expanded, z_prev_seq)\n",
        "        sigma_seq = torch.sqrt(sigma2_seq)\n",
        "\n",
        "        # Inverse Reparameterization: u = (z - mu) / sigma\n",
        "        u_local = (z_local - mu_seq) / (sigma_seq + 1e-6)\n",
        "\n",
        "        return [u_t, u_local]"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-09T03:55:39.287903Z",
          "iopub.execute_input": "2025-12-09T03:55:39.288540Z",
          "iopub.status.idle": "2025-12-09T03:55:39.296580Z",
          "shell.execute_reply.started": "2025-12-09T03:55:39.288514Z",
          "shell.execute_reply": "2025-12-09T03:55:39.295789Z"
        },
        "id": "7Y2NBmx3S3pN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CrossAttentionEBM(nn.Module):\n",
        "    \"\"\"\n",
        "    Unified EBM using Cross-Attention.\n",
        "    Can be used for EBM1 (Global) or EBM2 (Sequence) by changing seq_len.\n",
        "    Supports both single and sequence latent inputs and includes masked pooling.\n",
        "    \"\"\"\n",
        "    def __init__(self, z_dim, context_dim, hidden_dim=128, num_heads=4, layers=2, seq_len=1):\n",
        "        super().__init__()\n",
        "\n",
        "        # 1. Projections\n",
        "        self.z_proj = nn.Linear(z_dim, hidden_dim)\n",
        "        self.ctx_proj = nn.Sequential(\n",
        "            nn.Linear(context_dim, 512), # Maintain width\n",
        "            nn.GELU(),                         # Non-linearity\n",
        "            nn.LayerNorm(hidden_dim),          # Optional: Stabilizes energy magnitudes\n",
        "            nn.Linear(512, hidden_dim)           # Squeeze to scalar\n",
        "        )\n",
        "        self.time_proj = nn.Linear(1, hidden_dim)\n",
        "\n",
        "        # 2. Position Embedding (Only needed if seq_len > 1, e.g. for Local Latents)\n",
        "        if seq_len > 1:\n",
        "            self.pos_emb = nn.Parameter(torch.randn(1, seq_len, hidden_dim) * 0.02)\n",
        "        else:\n",
        "            self.pos_emb = None\n",
        "\n",
        "        # 3. Transformer Decoder Layers\n",
        "        # (Decoder = Self-Attn + Cross-Attn + FeedForward)\n",
        "        # Note: We use 'TransformerDecoder' because it has Cross-Attention built-in.\n",
        "        decoder_layer = nn.TransformerDecoderLayer(\n",
        "            d_model=hidden_dim,\n",
        "            nhead=num_heads,\n",
        "            batch_first=True,\n",
        "            norm_first=True # Usually stabilizes EBM training\n",
        "        )\n",
        "        self.transformer = nn.TransformerDecoder(decoder_layer, num_layers=layers)\n",
        "\n",
        "        # 4. Output Head\n",
        "        self.energy_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim), # Maintain width\n",
        "            nn.GELU(),                         # Non-linearity\n",
        "            nn.LayerNorm(hidden_dim),          # Optional: Stabilizes energy magnitudes\n",
        "            nn.Linear(hidden_dim, 1)           # Squeeze to scalar\n",
        "        )\n",
        "\n",
        "    def forward(self, z, context, t, context_mask=None, latent_word_mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            z: [Batch, seq_len, z_dim]\n",
        "            context: [Batch, context_len, context_dim]\n",
        "            t: [Batch, 1]\n",
        "\n",
        "            context_mask: [Batch, context_len]\n",
        "                          1 = Real Token, 0 = Padding\n",
        "\n",
        "            latent_word_mask: [Batch, max_sent, max_words] (from enc_wordMask)\n",
        "                              1 = Real Word, 0 = Padding\n",
        "                              Used to determine which SENTENCES are zombies.\n",
        "        \"\"\"\n",
        "\n",
        "        # --- 1. Prepare Masks ---\n",
        "\n",
        "        # A. Context Mask (for Cross-Attention)\n",
        "        # PyTorch expects True for \"PAD/IGNORE\".\n",
        "        # Your mask has 0 for Pad. So we invert it: (mask == 0) -> True.\n",
        "        if context_mask is not None:\n",
        "            memory_key_padding_mask = (context_mask == 0)\n",
        "        else:\n",
        "            memory_key_padding_mask = None\n",
        "\n",
        "        # B. Latent Mask (for Self-Attention & Pooling)\n",
        "        # Determine if a sentence is padding based on its FIRST word\n",
        "        tgt_key_padding_mask = None\n",
        "        sentence_mask_float = None\n",
        "\n",
        "        if latent_word_mask is not None and z.size(1) > 1:\n",
        "            # Extract the first word of every sentence: [B, Max_Sent]\n",
        "            # If first word is 0, sentence is 0.\n",
        "            sentence_mask = latent_word_mask[:, :, 0]\n",
        "\n",
        "            # Create boolean mask for Transformer (True = Ignore/Pad)\n",
        "            tgt_key_padding_mask = (sentence_mask == 0)\n",
        "\n",
        "            # Keep a float version (1.0 = Keep, 0.0 = Ignore) for pooling later\n",
        "            sentence_mask_float = sentence_mask.float().unsqueeze(-1) # [B, S, 1]\n",
        "\n",
        "        # --- 2. Embeddings ---\n",
        "        z_emb = self.z_proj(z) # [B, S, H]\n",
        "        t_emb = self.time_proj(t).unsqueeze(1)\n",
        "        z_input = z_emb + t_emb\n",
        "\n",
        "        if self.pos_emb is not None:\n",
        "            z_input = z_input + self.pos_emb\n",
        "\n",
        "        ctx_emb = self.ctx_proj(context) # [B, C, H]\n",
        "\n",
        "        # --- 3. Transformer (With Masks) ---\n",
        "        # tgt_key_padding_mask    -> z looking at z (Self-Attn)\n",
        "        # memory_key_padding_mask -> z looking at context (Cross-Attn)\n",
        "        out = self.transformer(\n",
        "            tgt=z_input,\n",
        "            memory=ctx_emb,\n",
        "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "            memory_key_padding_mask=memory_key_padding_mask\n",
        "        )\n",
        "\n",
        "        # --- 4. Masked Energy Pooling ---\n",
        "        if out.size(1) > 1:\n",
        "            if sentence_mask_float is not None:\n",
        "\n",
        "                # Zero out the energy of zombie sentences\n",
        "                masked_out = out * sentence_mask_float\n",
        "\n",
        "                # Sum valid energies\n",
        "                sum_out = masked_out.sum(dim=1) # [B, H]\n",
        "\n",
        "                # Count valid sentences (avoid div by zero)\n",
        "                count_valid = sentence_mask_float.sum(dim=1) # [B, 1]\n",
        "                count_valid = torch.clamp(count_valid, min=1.0)\n",
        "\n",
        "                out_pooled = sum_out / count_valid\n",
        "            else:\n",
        "                # Fallback if no mask provided\n",
        "                out_pooled = out.mean(dim=1)\n",
        "        else:\n",
        "            # For Global Latent (seq_len=1), just squeeze\n",
        "            out_pooled = out.squeeze(1)\n",
        "\n",
        "        return self.energy_head(out_pooled)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-09T03:39:06.834447Z",
          "iopub.execute_input": "2025-12-09T03:39:06.834700Z",
          "iopub.status.idle": "2025-12-09T03:39:06.845426Z",
          "shell.execute_reply.started": "2025-12-09T03:39:06.834682Z",
          "shell.execute_reply": "2025-12-09T03:39:06.844865Z"
        },
        "id": "mjfj1dx6S3pN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class HierarchicalTransformerEBM(nn.Module):\n",
        "    \"\"\"\n",
        "    Class to combine two CrossAttentionEBM instances: one for global latents (ebm1)\n",
        "    and one for local latents (ebm2).\n",
        "    \"\"\"\n",
        "    def __init__(self, dim_z1=128, dim_z2=128, dim_context=768):\n",
        "        super().__init__()\n",
        "\n",
        "        self.ebm1 = CrossAttentionEBM(dim_z1, dim_context,seq_len=1)\n",
        "        self.ebm2 = CrossAttentionEBM(dim_z2, dim_context,seq_len=11)\n",
        "\n",
        "    def forward(self, z_list, context, t):\n",
        "        z1, z2 = z_list\n",
        "\n",
        "        # EBM 1 Energy\n",
        "        energy1 = self.ebm1(z1, context, t, context_mask, latent_word_mask)\n",
        "\n",
        "        # EBM 2 Energy\n",
        "        energy2 = self.ebm2(z2, context, t, context_mask, latent_word_mask)\n",
        "\n",
        "        return energy1.sum() + energy2.sum()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-09T03:39:11.936932Z",
          "iopub.execute_input": "2025-12-09T03:39:11.937246Z",
          "iopub.status.idle": "2025-12-09T03:39:11.942052Z",
          "shell.execute_reply.started": "2025-12-09T03:39:11.937220Z",
          "shell.execute_reply": "2025-12-09T03:39:11.941233Z"
        },
        "id": "CFoGhVgsS3pN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    Optimized MLP for HVAE Encoder q(z|x).\n",
        "    1. Removes bottlenecks (Maintains width).\n",
        "    2. Uses GELU (Matches BERT/Transformer activations).\n",
        "    3. Implements Near-Zero Initialization to prevent early KL shock.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, latent_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        # 1. Maintain Width: Do not compress to //2 or //4 immediately.\n",
        "        # We want deep non-linearities, not compression.\n",
        "        hidden_dim = input_dim\n",
        "\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.ln1 = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.ln2 = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "        # Output layer\n",
        "        self.fc_out = nn.Linear(hidden_dim, 2 * latent_dim, bias=False)\n",
        "\n",
        "        # Match activation to DistilBERT/GPT-2 (GELU is smoother than ReLU)\n",
        "        self.activation = nn.GELU()\n",
        "\n",
        "        # --- CRITICAL: Near-Zero Initialization ---\n",
        "        # This ensures that at Step 0, the posterior q(z|x) is very close to N(0,1).\n",
        "        # This prevents the initial KL loss from being huge, which scares the\n",
        "        # optimizer into \"killing\" the latent variable immediately (Collapse).\n",
        "        torch.nn.init.normal_(self.fc_out.weight, mean=0.0, std=0.001)\n",
        "\n",
        "\n",
        "    def forward(self, h):\n",
        "        # Post-Norm architecture (standard for Transformers)\n",
        "        x = self.fc1(h)\n",
        "        x = self.ln1(x)\n",
        "        x = self.activation(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = self.ln2(x)\n",
        "        x = self.activation(x)\n",
        "\n",
        "        output = self.fc_out(x)\n",
        "\n",
        "        mu, raw_var_score = output.chunk(2, dim=-1)\n",
        "\n",
        "        # Robust Softplus\n",
        "        sigma2 = F.softplus(raw_var_score) + 1e-6\n",
        "\n",
        "        return mu, sigma2\n",
        "\n",
        "class HT_HVAE_InferenceNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    The Hierarchical Transformer Encoder (Inference Network) q(z|x).\n",
        "    Implements shared-parameter word-level and sentence-level Transformers.\n",
        "    \"\"\"\n",
        "    def __init__(self,hyperparams):\n",
        "        super().__init__()\n",
        "\n",
        "        self.latent_dim = hyperparams['latent_dim']\n",
        "        self.d_model = hyperparams['d_model']\n",
        "        self.vocab_size = hyperparams['vocab_size']\n",
        "        self.max_sentences = hyperparams['max_sentences']\n",
        "        self.max_words = hyperparams['max_words']\n",
        "        self.n_heads = hyperparams['encoder_heads']\n",
        "        self.dropout = hyperparams['encoder_dropout']\n",
        "        self.n_layers = hyperparams['encoder_layers']\n",
        "\n",
        "        self.word_encoder = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "        self.distilbert_dim = self.word_encoder.config.hidden_size\n",
        "        self.word_projection = nn.Linear(self.distilbert_dim, self.d_model)\n",
        "\n",
        "        self.sentence_position_embedding = nn.Embedding(self.max_sentences + 1, self.d_model)\n",
        "\n",
        "\n",
        "        # 2.2. Sentence-Level Transformer Encoder\n",
        "        # This is a separate Transformer stack for sentence-level attention.\n",
        "\n",
        "        sentence_encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=self.d_model,\n",
        "            nhead=self.n_heads,\n",
        "            dim_feedforward=4*self.d_model,\n",
        "            dropout=self.dropout,\n",
        "            batch_first=True,\n",
        "            # 1. Change activation from 'relu' (default) to 'gelu'\n",
        "            activation='gelu',\n",
        "            # 2. Enable Pre-Layer Normalization (Pre-LN)\n",
        "            norm_first=True\n",
        "        )\n",
        "        self.transformer_sentence = nn.TransformerEncoder(sentence_encoder_layer, num_layers=self.n_layers)\n",
        "\n",
        "        # 2.3. Latent Variable Networks (MLP)\n",
        "        # These are used to fit the Gaussian distribution parameters.\n",
        "\n",
        "        # MLP for Global Posterior q(zt | x): Learned from text-code H_s^0\n",
        "        self.mlp_global = MLPNetwork(self.d_model, self.latent_dim)\n",
        "        self.local_latent = self.latent_dim\n",
        "\n",
        "        # MLP for Local Posterior q(zi | xi): Learned from sentence-code H_w^i\n",
        "        # This MLP MUST SHARE PARAMETERS across all sentences.\n",
        "        self.mlp_local = MLPNetwork(self.d_model, self.local_latent)\n",
        "        self.doc_token = nn.Parameter(torch.randn(1, 1, self.d_model))\n",
        "\n",
        "\n",
        "    def forward(self, input_ids, word_mask):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_ids (torch.LongTensor): Input tensor of shape\n",
        "                (batch_size, MAX_SENTENCES, MAX_WORDS).\n",
        "            word_mask (torch.BoolTensor): Mask tensor of shape\n",
        "                (batch_size, MAX_SENTENCES, MAX_WORDS). True for padded tokens.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (mu_t, sigma2_t, mu_i_list, sigma2_i_list)\n",
        "        \"\"\"\n",
        "        # word mask is 1 for real tokens, 0 for pad\n",
        "        batch_size, max_sentences, max_words = input_ids.shape\n",
        "\n",
        "        # --- 2.1. Word-Level Transformer Encoding (Shared) ---\n",
        "\n",
        "        # Reshape to treat all sentences as one batch for parameter sharing\n",
        "        # (batch_size * MAX_SENTENCES, MAX_WORDS)\n",
        "        flat_input_ids = input_ids.view(-1, max_words)\n",
        "        flat_attention_mask = word_mask.view(-1, max_words)\n",
        "        sentence_flat_word_mask = (word_mask.view(-1, max_words) == 0)\n",
        "\n",
        "\n",
        "\n",
        "        # PyTorch TransformerEncoder expects the mask to be True for elements that SHOULD be IGNORED (padded)\n",
        "        # and of shape (B, S). The mask must be the *attention mask* (source key padding mask).\n",
        "        # Mask shape for transformer: (B*N, S)\n",
        "        # Note: input_ids == 0 is often used for padding in simple setups.\n",
        "        # Here we use the provided word_mask.\n",
        "\n",
        "        distilbert_output = self.word_encoder(\n",
        "            input_ids=flat_input_ids,\n",
        "            attention_mask=flat_attention_mask\n",
        "        )\n",
        "\n",
        "        cls_embeddings = distilbert_output.last_hidden_state[:, 0, :]\n",
        "\n",
        "        # Extract the Sentence Code (Hw_0): (B*N, D)\n",
        "        # Hw_0 is the representation of the first token (usually <BOS> or the first word)\n",
        "        H_w_0_flat = self.word_projection(cls_embeddings)\n",
        "\n",
        "        is_padding_sentence = sentence_flat_word_mask[:, 0]\n",
        "\n",
        "        # 2. \"Firewall\": Replace NaNs with 0.0 immediately\n",
        "        # This protects BOTH the MLP and the Sentence Transformer downstream\n",
        "        H_w_0_flat = H_w_0_flat.masked_fill(is_padding_sentence.unsqueeze(1), 0.0)\n",
        "\n",
        "        # Reshape back: (batch_size, MAX_SENTENCES, D)\n",
        "        H_w_0 = H_w_0_flat.view(batch_size, max_sentences, self.d_model)\n",
        "\n",
        "        # --- 2.3. Local Posterior (q(zi | xi)) using Shared MLP ---\n",
        "\n",
        "        # Calculate local distribution parameters from Hw_0\n",
        "        # The mlp_local shares parameters because it is called on all Hw_0_flat\n",
        "        mu_i_flat, sigma2_i_flat = self.mlp_local(H_w_0_flat)\n",
        "\n",
        "        # Reshape back to (batch_size, MAX_SENTENCES, LATENT_DIM)\n",
        "        mu_i = mu_i_flat.view(batch_size, max_sentences, self.local_latent)\n",
        "        sigma2_i = sigma2_i_flat.view(batch_size, max_sentences, self.local_latent)\n",
        "\n",
        "        # --- 2.2. Sentence-Level Transformer Encoding ---\n",
        "\n",
        "        # Add sentence position codes to the sentence-codes Hw_0\n",
        "        # We assume position codes 1 to MAX_SENTENCES are used, and 0 is reserved for H_s^0 position.\n",
        "        # Create position IDs: 1, 2, 3...\n",
        "\n",
        "        batch_doc_token = self.doc_token.expand(batch_size, -1, -1)\n",
        "        H_sen_input_ = torch.cat([batch_doc_token, H_w_0], dim=1)\n",
        "        position_ids = torch.arange(0, max_sentences + 1, device=input_ids.device)\n",
        "        position_embeddings = self.sentence_position_embedding(position_ids) # (N, D)\n",
        "        position_embeddings = position_embeddings.unsqueeze(0).expand(batch_size, -1, -1) # (B, N, D)\n",
        "\n",
        "        H_sen_input = H_sen_input_ + position_embeddings\n",
        "\n",
        "        # Need a sentence-level padding mask\n",
        "        # Assuming sentence padding is where all words are 0/padded (i.e., word_mask[:, :, 0] is True)\n",
        "        sentence_mask = (word_mask[:, :, 0] == 0) # (B, N)\n",
        "\n",
        "        doc_mask = torch.zeros((batch_size, 1), dtype=torch.bool, device=input_ids.device)\n",
        "\n",
        "        full_sentence_mask = torch.cat([doc_mask, sentence_mask], dim=1)\n",
        "\n",
        "        # Sentence-Level Encoding (Hs): (B, N, D)\n",
        "        H_s = self.transformer_sentence(\n",
        "            src=H_sen_input,\n",
        "            src_key_padding_mask=full_sentence_mask\n",
        "        )\n",
        "\n",
        "        H_s_0 = H_s[:, 0, :] # (B, D)\n",
        "\n",
        "        # --- 2.3. Global Posterior (q(zt | x)) ---\n",
        "\n",
        "        # Calculate global distribution parameters from H_s^0\n",
        "        mu_t, sigma2_t = self.mlp_global(H_s_0) # (B, LATENT_DIM)\n",
        "\n",
        "        return mu_t, sigma2_t, mu_i, sigma2_i\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-09T03:39:14.512925Z",
          "iopub.execute_input": "2025-12-09T03:39:14.513496Z",
          "iopub.status.idle": "2025-12-09T03:39:14.528351Z",
          "shell.execute_reply.started": "2025-12-09T03:39:14.513472Z",
          "shell.execute_reply": "2025-12-09T03:39:14.527656Z"
        },
        "id": "PwvYGiPHS3pN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPNetworkForPrior(nn.Module):\n",
        "    \"\"\"\n",
        "    Optimized Prior Network p(z_i | z_t, z_i-1).\n",
        "    1. Widened layers (No bottleneck).\n",
        "    2. Context Dropout (Forces reliance on z_t).\n",
        "    3. Near-Zero Init (Prevents initial KL explosion).\n",
        "    \"\"\"\n",
        "    def __init__(self, latent_dim, context_dropout_rate=0.05):\n",
        "        super().__init__()\n",
        "        self.context_dropout_rate = context_dropout_rate\n",
        "\n",
        "        # Input: Global (32) + Local (32) = 96\n",
        "        input_dim = latent_dim + latent_dim\n",
        "\n",
        "        # 1. Maintain Width:\n",
        "        # Instead of compressing, we project up or keep equal.\n",
        "        # 128 gives enough capacity to mix Global and Previous-Local info.\n",
        "        hidden_dim = 2*input_dim\n",
        "\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.ln1 = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.ln2 = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "        self.fc_out = nn.Linear(hidden_dim, 2 * latent_dim, bias=False) # Outputs (mu, var_param)\n",
        "\n",
        "        self.activation = nn.GELU()\n",
        "\n",
        "        # --- CRITICAL: Near-Zero Initialization ---\n",
        "        # Initialize output to be very close to N(0, 1) parameters.\n",
        "        # mu -> 0, sigma_param -> 0 (which becomes softplus(0) ~ 0.69)\n",
        "        # This matches the initialization of your Encoder.\n",
        "        torch.nn.init.normal_(self.fc_out.weight, mean=0.0, std=0.001)\n",
        "        # torch.nn.init.constant_(self.fc_out.bias, 0)\n",
        "\n",
        "    def forward(self, z_t, z_i_minus_1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            z_t (Tensor): Global latent (B, D_global)\n",
        "            z_i_minus_1 (Tensor): Previous local latent (B, D_local)\n",
        "        \"\"\"\n",
        "\n",
        "        # --- 2. Context Dropout (The \"Firewall\") ---\n",
        "        # Randomly zero out the previous sentence information.\n",
        "        # This forces the network to look at z_t to guess the current sentence state.\n",
        "        if self.training and self.context_dropout_rate > 0:\n",
        "            mask_prob = torch.rand(z_t.shape[0], 1, device=z_t.device)\n",
        "            # If random > rate, keep signal (1.0). Else drop (0.0).\n",
        "            keep_mask = (mask_prob > self.context_dropout_rate).float()\n",
        "            z_i_minus_1 = z_i_minus_1 * keep_mask\n",
        "\n",
        "        # Concatenate inputs\n",
        "        h = torch.cat([z_t, z_i_minus_1], dim=-1)\n",
        "\n",
        "        # Block 1\n",
        "        x = self.fc1(h)\n",
        "        x = self.ln1(x)\n",
        "        x = self.activation(x)\n",
        "\n",
        "        # Block 2\n",
        "        x = self.fc2(x)\n",
        "        x = self.ln2(x)\n",
        "        x = self.activation(x)\n",
        "\n",
        "        # Output Head\n",
        "        output = self.fc_out(x)\n",
        "\n",
        "        mu, raw_var_score = output.chunk(2, dim=-1)\n",
        "\n",
        "        # Robust Softplus\n",
        "        sigma2 = F.softplus(raw_var_score) + 1e-6\n",
        "\n",
        "        return mu, sigma2\n",
        "\n",
        "class HT_HVAE_GenerativeNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    The Hierarchical VAE Generative Network (Decoder) p(x|z).\n",
        "    Uses GRU for sentence planning and modified GPT-2 for word generation.\n",
        "    \"\"\"\n",
        "    def __init__(self,hyperparams):\n",
        "        super().__init__()\n",
        "\n",
        "        # 3.1. Global Prior: Standard Gaussian N(0, I) is defined conceptually.\n",
        "        self.latent_dim = hyperparams['latent_dim']\n",
        "        self.local_latent = self.latent_dim\n",
        "        self.d_model = hyperparams['d_model']\n",
        "        self.gpt2_model_name = hyperparams['gpt2_model_name']\n",
        "        self.vocab_size = hyperparams['vocab_size']\n",
        "        self.gru_layers = hyperparams['gru_layers']\n",
        "\n",
        "        # 3.2. Local Prior Network (HMM integration)\n",
        "        self.prior_mlp = MLPNetworkForPrior(self.latent_dim)\n",
        "        self.gru_initial_projection = nn.Linear(self.latent_dim, self.d_model)\n",
        "\n",
        "        # 3.3. Sentence-Level Decoder (GRU)\n",
        "        # Input size is LATENT_DIM (z_i), hidden size matches D_MODEL (for h_i), 1 layer.\n",
        "        self.gru = nn.GRU(\n",
        "        input_size=self.local_latent,\n",
        "        hidden_size=self.d_model,\n",
        "        num_layers=self.gru_layers,\n",
        "        batch_first=True\n",
        "      )\n",
        "\n",
        "        # 3.4. Word-Level Decoder (Modified GPT-2)\n",
        "\n",
        "        # Load pre-trained GPT-2 and modify its components for the VAE structure.\n",
        "        # We load the full model to utilize its weights.\n",
        "        self.gpt2_model = GPT2Model.from_pretrained(self.gpt2_model_name)\n",
        "\n",
        "        self.gpt2_model.resize_token_embeddings(self.vocab_size)\n",
        "\n",
        "        gpt2_hidden_size = self.gpt2_model.config.n_embd\n",
        "\n",
        "        self.global_projector = nn.Linear(self.latent_dim, gpt2_hidden_size)\n",
        "\n",
        "\n",
        "        # Define the projection layer here so weights are saved and trained\n",
        "        self.latent_projection = nn.Linear(self.d_model, gpt2_hidden_size)\n",
        "\n",
        "        # Set the embedding layer from GPT-2\n",
        "        self.word_embedding = self.gpt2_model.wte\n",
        "\n",
        "        self.word_dropout_rate = hyperparams['word_dropout_rate']\n",
        "        self.plan_dropout_rate = hyperparams['plan_dropout_rate']\n",
        "        self.mask_token_id = hyperparams.get('mask_token_id', self.gpt2_model.config.eos_token_id)\n",
        "\n",
        "\n",
        "        # Get GPT-2's vocabulary head (unembedder)\n",
        "        # In AutoModelForCausalLM, this is usually the same as wte.weight, but we need a specific layer\n",
        "        # Since we modify the input to the final linear layer, we redefine the classification head.\n",
        "\n",
        "        # Define the final linear layer: Input is [Hin || hi]\n",
        "        # Output is the vocabulary size (Logits)\n",
        "        self.final_linear = nn.Linear(gpt2_hidden_size + self.d_model, self.vocab_size, bias = False)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, input_ids,word_mask, z_t, z_i_samples, mu_i_prior=None, log_sigma2_i_prior=None):\n",
        "        \"\"\"\n",
        "        Processes text sequentially for reconstruction loss calculation (training mode).\n",
        "\n",
        "        Args:\n",
        "            input_ids (torch.LongTensor): Word IDs of shape (B, N, S).\n",
        "            z_t (torch.Tensor): Sampled global latent variable (B, LATENT_DIM).\n",
        "            z_i_samples (torch.Tensor): Sampled local latent variables (B, N, LATENT_DIM).\n",
        "            mu_i_prior (torch.Tensor, optional): Pre-calculated prior mean for KL divergence.\n",
        "            log_sigma2_i_prior (torch.Tensor, optional): Pre-calculated prior log-variance for KL divergence.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (reconstruction_logits, mu_i_prior, sigma2_i_prior)\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, max_sentences, max_words = input_ids.shape\n",
        "        global_drop_prob = 0.5\n",
        "\n",
        "        if self.training and global_drop_prob > 0:\n",
        "          # Create mask: 1 = Keep, 0 = Drop\n",
        "          mask_prob = torch.rand(z_t.size(0), 1, device=z_t.device)\n",
        "          keep_global_mask = (mask_prob > global_drop_prob).float()\n",
        "\n",
        "          # Apply to z_t (This effectively zeroes h_0 for the GRU and the token for GPT)\n",
        "          z_t_masked = z_t * keep_global_mask\n",
        "        else:\n",
        "          z_t_masked = z_t\n",
        "\n",
        "\n",
        "\n",
        "        # --- 3.2. Sentence-Level Decoder (GRU) ---\n",
        "\n",
        "        # The GRU processes the sequence of local latent variables (z_i_samples)\n",
        "        # Sequence of z_i: (B, N, LATENT_DIM)\n",
        "\n",
        "        # Initial hidden state for the GRU (often zeros, or derived from z_t if desired,\n",
        "        # but the paper just uses z_i as input)\n",
        "\n",
        "        # GRU output: plan_vectors (h_i) is the sequence of outputs for each z_i\n",
        "\n",
        "        h_0_projected = self.gru_initial_projection(z_t_masked)\n",
        "        h_0_projected = h_0_projected.unsqueeze(0)\n",
        "        h_0 = h_0_projected.repeat(self.gru.num_layers, 1, 1) # Shape: (1, B, D_MODEL) ho is expected num_gru layers batch size hidden size\n",
        "\n",
        "        plan_vectors, _ = self.gru(z_i_samples, h_0) # (B, N, D_MODEL)\n",
        "\n",
        "        # --- 3.2. Local Prior Network calculation (for KL divergence) ---\n",
        "\n",
        "        # We need z_i-1, which is the previous sampled local latent variable.\n",
        "        # Create z_i-1 by shifting z_i_samples:\n",
        "        z_i_minus_1 = torch.cat([\n",
        "            torch.zeros_like(z_i_samples[:, :1, :]), # Use zero vector for z_i-1 of the first sentence\n",
        "            z_i_samples[:, :-1, :]\n",
        "        ], dim=1).view(-1, self.local_latent) # Flatten to (B*N, D)\n",
        "\n",
        "        # Flatten z_t and z_i_samples for MLP processing\n",
        "        z_t_flat = z_t_masked.unsqueeze(1).repeat(1, max_sentences, 1).view(-1, self.latent_dim) # (B*N, D)\n",
        "\n",
        "        # Calculate the conditional prior parameters for all sentences\n",
        "        mu_i_prior, sigma2_i_prior = self.prior_mlp(z_t_flat, z_i_minus_1)\n",
        "        # Reshape to (B, N, LATENT_DIM)\n",
        "        mu_i_prior = mu_i_prior.view(batch_size, max_sentences, self.local_latent)\n",
        "        sigma2_i_prior = sigma2_i_prior.view(batch_size, max_sentences, self.local_latent)\n",
        "\n",
        "        # --- 3.4. Word-Level Decoder (Modified GPT-2) ---\n",
        "\n",
        "        # Reshape everything to feed into the word-level loop\n",
        "        flat_input_ids = input_ids.view(-1, max_words) # (B*N, S)\n",
        "        flat_plan_vectors = plan_vectors.reshape(-1, self.d_model) # (B*N, D) (h_i for each sentence)\n",
        "\n",
        "        # 1. Word Embeddings (e_ij): (B*N, S, D)\n",
        "        if self.training and self.word_dropout_rate > 0:\n",
        "            # 1. Create a mask of random probabilities\n",
        "            rand_mask = torch.rand(flat_input_ids.shape, device=input_ids.device)\n",
        "\n",
        "            # 2. Identify tokens to drop (probability < rate)\n",
        "            # We also ensure we do NOT drop padding tokens (if 0 is your pad)\n",
        "            # Assuming word_mask indicates valid words (1) and padding (0)\n",
        "            flat_word_mask_bool = word_mask.view(-1, max_words).bool()\n",
        "\n",
        "            # Drop mask: True where we should replace with UNK\n",
        "            drop_mask = (rand_mask < self.word_dropout_rate) & flat_word_mask_bool\n",
        "\n",
        "            # 3. Create a clone to avoid in-place modification errors\n",
        "            input_ids_for_decoder = flat_input_ids.clone()\n",
        "            input_ids_for_decoder[drop_mask] = self.mask_token_id\n",
        "\n",
        "        else:\n",
        "            # No dropout during evaluation\n",
        "            input_ids_for_decoder = flat_input_ids\n",
        "\n",
        "\n",
        "\n",
        "        inputs_embeds = self.gpt2_model.wte(input_ids_for_decoder)\n",
        "\n",
        "        # 2. Project Latents (Plan Vectors)\n",
        "        # Ensure self.latent_projection maps from D_MODEL -> GPT_Hidden_Dim (e.g., 768)\n",
        "        projected_latents = self.latent_projection(flat_plan_vectors) # (B*N, GPT_Hidden)\n",
        "\n",
        "        # 3. Expand Latents to match Sequence Length\n",
        "        # (B*N, 1, GPT_Hidden) -> (B*N, S, GPT_Hidden)\n",
        "        latent_embeds = projected_latents.unsqueeze(1).expand(-1, max_words, -1)\n",
        "\n",
        "        # 4. Additive Conditioning\n",
        "        # The latent information is now fused into the input representation\n",
        "\n",
        "        if self.training and self.plan_dropout_rate > 0:\n",
        "            # We drop the ENTIRE plan vector for a sequence, not just random dimensions\n",
        "            # Shape: (B*N, 1, 1) to broadcast over Sequence and Hidden dims\n",
        "            p_mask = torch.rand(latent_embeds.size(0), 1, 1, device=latent_embeds.device)\n",
        "\n",
        "            # Create boolean mask: 1 = Keep, 0 = Drop\n",
        "            # If random number > dropout_rate, we keep the plan\n",
        "            keep_plan_mask = (p_mask > self.plan_dropout_rate).float()\n",
        "\n",
        "            # Apply mask: latent_embeds becomes all zeros where mask is 0\n",
        "            latent_embeds = latent_embeds * keep_plan_mask\n",
        "\n",
        "        inputs_embeds = inputs_embeds + latent_embeds\n",
        "\n",
        "        # 5. Forward Pass\n",
        "        # Note: We pass 'inputs_embeds' instead of 'input_ids'.\n",
        "        # GPT-2 will automatically add Position Embeddings (wpe) to this internally.\n",
        "        z_t_expanded = z_t.unsqueeze(1).expand(-1, max_sentences, -1).reshape(-1, self.latent_dim)\n",
        "\n",
        "        # 2. Project z_t to GPT hidden dimension and add sequence dim\n",
        "        # Requires: self.global_projector = nn.Linear(latent_dim, gpt_hidden_dim)\n",
        "        z_t_emb = self.global_projector(z_t_expanded).unsqueeze(1) # (B*N, 1, GPT_Hidden)\n",
        "\n",
        "        # 3. Concatenate global token to the beginning of inputs\n",
        "        inputs_embeds = torch.cat([z_t_emb, inputs_embeds], dim=1) # (B*N, S+1, GPT_Hidden)\n",
        "\n",
        "        # 4. Adjust Attention Mask\n",
        "        flat_word_mask = word_mask.view(-1, max_words)\n",
        "        # Create a mask of 1s for the global token\n",
        "        global_mask_col = torch.ones((flat_word_mask.size(0), 1), device=flat_word_mask.device)\n",
        "        # Concatenate mask: (B*N, S+1)\n",
        "        extended_attention_mask = torch.cat([global_mask_col, flat_word_mask], dim=1)\n",
        "\n",
        "        gpt2_output = self.gpt2_model(\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            attention_mask=extended_attention_mask\n",
        "        )\n",
        "        # Extract the hidden states from the last layer (H_in)\n",
        "        H_in = gpt2_output.last_hidden_state # (B*N, S, D)\n",
        "\n",
        "        plan_unsqueezed = flat_plan_vectors.unsqueeze(1)\n",
        "\n",
        "        # 2. Expand to match the sequence length 'S' of H_in: (B*N, 1, D) -> (B*N, S, D)\n",
        "        # Using .expand() is more memory efficient than .repeat()\n",
        "        copied_plan_vector = plan_unsqueezed.expand(-1, H_in.size(1), -1)\n",
        "\n",
        "        # 3. Concatenate: Result shape is (B*N, S, GPT_Hidden + Plan_Dim)\n",
        "        final_input = torch.cat([H_in, copied_plan_vector], dim=-1)\n",
        "\n",
        "\n",
        "        # Calculate Logits (Final output for reconstruction loss)\n",
        "        reconstruction_logits = self.final_linear(final_input) # (B*N, S, VOCAB_SIZE)\n",
        "\n",
        "        # Reshape logits back to (B, N, S, VOCAB_SIZE) for loss calculation\n",
        "        reconstruction_logits = reconstruction_logits.view(\n",
        "            batch_size, max_sentences, max_words + 1, self.vocab_size\n",
        "        )\n",
        "\n",
        "        return reconstruction_logits, mu_i_prior, sigma2_i_prior\n",
        "\n",
        ""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-09T03:39:20.032930Z",
          "iopub.execute_input": "2025-12-09T03:39:20.033644Z",
          "iopub.status.idle": "2025-12-09T03:39:20.054155Z",
          "shell.execute_reply.started": "2025-12-09T03:39:20.033612Z",
          "shell.execute_reply": "2025-12-09T03:39:20.053594Z"
        },
        "id": "Xje5pASwS3pO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import wandb\n",
        "\n",
        "def load_models_for_inference(\n",
        "    artifact_path,\n",
        "    inference_net,\n",
        "    generative_net,\n",
        "    filename=\"hvae_checkpoint.pth\",\n",
        "    device=\"cuda\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Downloads the artifact and loads ONLY the model weights.\n",
        "    Ignores optimizer, scheduler, and training state.\n",
        "    \"\"\"\n",
        "    print(f\"Fetching inference models from: {artifact_path}\")\n",
        "\n",
        "    # 1. Initialize WandB if not active (required to download artifacts)\n",
        "    if wandb.run is None:\n",
        "        # You can use \"anonymous\" or your specific entity/project\n",
        "        wandb.init(project=\"my_project\", job_type=\"inference\", mode=\"online\")\n",
        "\n",
        "    # 2. Download the artifact\n",
        "    artifact = wandb.use_artifact(artifact_path, type='model')\n",
        "    artifact_dir = artifact.download()\n",
        "    filepath = os.path.join(artifact_dir, filename)\n",
        "\n",
        "    # 3. Load the dictionary\n",
        "    # We load the whole file into RAM, but we only extract what we need\n",
        "    if torch.cuda.is_available() and device == 'cuda':\n",
        "        checkpoint = torch.load(filepath)\n",
        "    else:\n",
        "        checkpoint = torch.load(filepath, map_location=torch.device('cpu'))\n",
        "\n",
        "    # 4. Load ONLY the model states\n",
        "    # strict=True ensures the keys match exactly (good for safety)\n",
        "    inference_net.load_state_dict(checkpoint['inference_state_dict'])\n",
        "    generative_net.load_state_dict(checkpoint['generative_state_dict'])\n",
        "\n",
        "    # 5. Set to Evaluation Mode\n",
        "    # Critical: This turns off Dropout and fixes Batch Norm layers\n",
        "    inference_net.eval()\n",
        "    generative_net.eval()\n",
        "\n",
        "    print(f\"Successfully loaded models from Epoch {checkpoint.get('epoch', 'Unknown')}\")\n",
        "\n",
        "    return inference_net, generative_net\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-09T03:39:29.408772Z",
          "iopub.execute_input": "2025-12-09T03:39:29.409324Z",
          "iopub.status.idle": "2025-12-09T03:39:32.555419Z",
          "shell.execute_reply.started": "2025-12-09T03:39:29.409300Z",
          "shell.execute_reply": "2025-12-09T03:39:32.554894Z"
        },
        "id": "moe7o4AyS3pO",
        "outputId": "5829549e-ff6e-4fcd-e65f-fe3880f09e13"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "hyperParams = {\n",
        "    # --- Architecture Constraints ---\n",
        "    'gpt2_model_name': 'gpt2',   # Start with 'gpt2' (124M params). Use 'gpt2-medium' only if you have high VRAM.\n",
        "    'd_model': 768,              # MUST be 768 for 'gpt2', 1024 for 'gpt2-medium', 1280 for 'gpt2-large'.\n",
        "    'vocab_size': 50257,         # Standard GPT-2 vocabulary size.\n",
        "\n",
        "    # --- Latent Space ---\n",
        "    'latent_dim': 32,           # 32-128 is standard. Too large = posterior collapse; Too small = poor reconstruction.\n",
        "\n",
        "    # --- Data Dimensions (Abstract specific) ---\n",
        "    'max_sentences': 11,         # Average abstract is 5-8 sentences; 10 provides a safety buffer.\n",
        "    'max_words': 50,             # Average sentence is 20-30 words; 50 covers outliers.\n",
        "\n",
        "    # --- Inference Network (Encoder) ---\n",
        "    'encoder_layers': 2,         # Keep encoder shallow (2-4) so the heavy lifting happens in the decoder/latent space.\n",
        "    'encoder_heads': 8,          # Standard for d_model=768 (768 / 8 = 96 dim per head).\n",
        "    'encoder_dropout': 0.1,      # Standard transformer dropout.\n",
        "\n",
        "    # --- Sentence Decoder (GRU) ---\n",
        "    'gru_layers': 1,             # 1 layer is sufficient for the high-level plan; more layers add unnecessary complexity.\n",
        "\n",
        "    # --- Special Tokens ---\n",
        "    'pad_index': 50256,          # GPT-2 uses EOS (50256) as PAD by default unless you added a new token.\n",
        "    'word_dropout_rate' : 0.5,\n",
        "    'plan_dropout_rate':0.15,\n",
        "    'mask_token_id':10\n",
        "}\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-09T03:43:03.246231Z",
          "iopub.execute_input": "2025-12-09T03:43:03.246932Z",
          "iopub.status.idle": "2025-12-09T03:43:03.251623Z",
          "shell.execute_reply.started": "2025-12-09T03:43:03.246910Z",
          "shell.execute_reply": "2025-12-09T03:43:03.251026Z"
        },
        "id": "b1TSnlOHS3pO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, DistilBertTokenizer\n",
        "\n",
        "# 1. Load standard tokenizers\n",
        "dec_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "enc_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# 2. Add Special Tokens\n",
        "# We use 'additional_special_tokens' for custom tokens like EOT\n",
        "special_tokens_dict = {\n",
        "    'bos_token': '<|BOS|>',\n",
        "    'eos_token': '<|EOS|>',\n",
        "    'pad_token': '<|PAD|>',\n",
        "    'mask_token': '<|MASK|>',\n",
        "    'additional_special_tokens': ['<|EOT|>']\n",
        "}\n",
        "\n",
        "# This returns the number of added tokens\n",
        "num_added_toks = dec_tokenizer.add_special_tokens(special_tokens_dict)\n",
        "\n",
        "# 3. Get IDs\n",
        "# Standard attributes exist for bos/eos/pad/mask\n",
        "pad_idx = dec_tokenizer.pad_token_id\n",
        "eos_idx = dec_tokenizer.eos_token_id\n",
        "mask_idx = dec_tokenizer.mask_token_id\n",
        "\n",
        "# For EOT, we must look it up manually since .eot_token_id doesn't exist\n",
        "eot_token_id = dec_tokenizer.convert_tokens_to_ids('<|EOT|>')\n",
        "\n",
        "new_vocab_size = len(dec_tokenizer)\n",
        "\n",
        "print(f\"New Vocab Size: {new_vocab_size}\")\n",
        "print(f\"PAD ID: {pad_idx} | EOS ID: {eos_idx}\")\n",
        "print(f\"MASK ID: {mask_idx} | EOT ID: {eot_token_id}\")\n",
        "\n",
        "# 4. Update hyperparameters\n",
        "hyperParams['vocab_size'] = new_vocab_size\n",
        "hyperParams['pad_index'] = pad_idx\n",
        "hyperParams['mask_token_id'] = mask_idx\n",
        "hyperParams['eot_token_id'] = eot_token_id"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-09T03:48:22.756886Z",
          "iopub.execute_input": "2025-12-09T03:48:22.757645Z",
          "iopub.status.idle": "2025-12-09T03:48:25.537659Z",
          "shell.execute_reply.started": "2025-12-09T03:48:22.757619Z",
          "shell.execute_reply": "2025-12-09T03:48:25.536878Z"
        },
        "colab": {
          "referenced_widgets": [
            "cd3812c538144c90bd35f0d24fad3018",
            "1c45f825634744519d5a1cda2c15aa5b",
            "d5cb702d9b88424f99bbad5bab5fa101",
            "9ccc55fb40ad46759fb741fd465771f5"
          ]
        },
        "id": "lmnJ3Mj6S3pO",
        "outputId": "6071581a-be3b-48a2-fb69-af37980060f8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cd3812c538144c90bd35f0d24fad3018"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1c45f825634744519d5a1cda2c15aa5b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d5cb702d9b88424f99bbad5bab5fa101"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9ccc55fb40ad46759fb741fd465771f5"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "New Vocab Size: 50262\nPAD ID: 50259 | EOS ID: 50258\nMASK ID: 50260 | EOT ID: 50261\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "inference_net = HT_HVAE_InferenceNetwork(hyperParams).to(device)\n",
        "generative_net = HT_HVAE_GenerativeNetwork(hyperParams).to(device)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-09T03:48:31.285275Z",
          "iopub.execute_input": "2025-12-09T03:48:31.285968Z",
          "iopub.status.idle": "2025-12-09T03:48:36.574859Z",
          "shell.execute_reply.started": "2025-12-09T03:48:31.285945Z",
          "shell.execute_reply": "2025-12-09T03:48:36.574286Z"
        },
        "id": "g7v5gOBWS3pO",
        "outputId": "55f83d86-4b94-4d61-91bd-3d33dfa42378"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(\n",
        "    project=\"HDHEBM\",\n",
        "    config=hyperParams,\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-09T03:43:55.905116Z",
          "iopub.execute_input": "2025-12-09T03:43:55.905769Z",
          "iopub.status.idle": "2025-12-09T03:44:04.153238Z",
          "shell.execute_reply.started": "2025-12-09T03:43:55.905747Z",
          "shell.execute_reply": "2025-12-09T03:44:04.152503Z"
        },
        "id": "W969intyS3pO",
        "outputId": "388e9e19-bf87-4a47-caef-13572f5f7370"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.21.0"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/kaggle/working/wandb/run-20251209_034355-1m4mp42n</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/yasir-alam14/HDHEBM/runs/1m4mp42n' target=\"_blank\">crisp-dream-1</a></strong> to <a href='https://wandb.ai/yasir-alam14/HDHEBM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/yasir-alam14/HDHEBM' target=\"_blank\">https://wandb.ai/yasir-alam14/HDHEBM</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/yasir-alam14/HDHEBM/runs/1m4mp42n' target=\"_blank\">https://wandb.ai/yasir-alam14/HDHEBM/runs/1m4mp42n</a>"
          },
          "metadata": {}
        },
        {
          "execution_count": 41,
          "output_type": "execute_result",
          "data": {
            "text/html": "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/yasir-alam14/HDHEBM/runs/1m4mp42n?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>",
            "text/plain": "<wandb.sdk.wandb_run.Run at 0x7b3436b68410>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Load weights\n",
        "\n",
        "load_models_for_inference(\n",
        "    artifact_path=\"yasir-alam14/HVAE-distilbert_plan_masking_full/hvae-model:v6\",\n",
        "    inference_net=inference_net,\n",
        "    generative_net=generative_net\n",
        ")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-09T03:48:41.326212Z",
          "iopub.execute_input": "2025-12-09T03:48:41.326878Z",
          "iopub.status.idle": "2025-12-09T03:48:52.664423Z",
          "shell.execute_reply.started": "2025-12-09T03:48:41.326855Z",
          "shell.execute_reply": "2025-12-09T03:48:52.663722Z"
        },
        "id": "yZdHQaE0S3pO",
        "outputId": "0956f0bf-8434-4eb0-b6de-3dbf44f2399d"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Fetching inference models from: yasir-alam14/HVAE-distilbert_plan_masking_full/hvae-model:v6\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact hvae-model:v6, 2895.48MB. 1 files... \n\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \nDone. 0:0:7.1 (408.3MB/s)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Successfully loaded models from Epoch 34\n",
          "output_type": "stream"
        },
        {
          "execution_count": 45,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(HT_HVAE_InferenceNetwork(\n   (word_encoder): DistilBertModel(\n     (embeddings): Embeddings(\n       (word_embeddings): Embedding(30522, 768, padding_idx=0)\n       (position_embeddings): Embedding(512, 768)\n       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n       (dropout): Dropout(p=0.1, inplace=False)\n     )\n     (transformer): Transformer(\n       (layer): ModuleList(\n         (0-5): 6 x TransformerBlock(\n           (attention): DistilBertSdpaAttention(\n             (dropout): Dropout(p=0.1, inplace=False)\n             (q_lin): Linear(in_features=768, out_features=768, bias=True)\n             (k_lin): Linear(in_features=768, out_features=768, bias=True)\n             (v_lin): Linear(in_features=768, out_features=768, bias=True)\n             (out_lin): Linear(in_features=768, out_features=768, bias=True)\n           )\n           (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n           (ffn): FFN(\n             (dropout): Dropout(p=0.1, inplace=False)\n             (lin1): Linear(in_features=768, out_features=3072, bias=True)\n             (lin2): Linear(in_features=3072, out_features=768, bias=True)\n             (activation): GELUActivation()\n           )\n           (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n         )\n       )\n     )\n   )\n   (word_projection): Linear(in_features=768, out_features=768, bias=True)\n   (sentence_position_embedding): Embedding(12, 768)\n   (transformer_sentence): TransformerEncoder(\n     (layers): ModuleList(\n       (0-1): 2 x TransformerEncoderLayer(\n         (self_attn): MultiheadAttention(\n           (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n         )\n         (linear1): Linear(in_features=768, out_features=3072, bias=True)\n         (dropout): Dropout(p=0.1, inplace=False)\n         (linear2): Linear(in_features=3072, out_features=768, bias=True)\n         (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n         (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n         (dropout1): Dropout(p=0.1, inplace=False)\n         (dropout2): Dropout(p=0.1, inplace=False)\n       )\n     )\n   )\n   (mlp_global): MLPNetwork(\n     (fc1): Linear(in_features=768, out_features=768, bias=True)\n     (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n     (fc2): Linear(in_features=768, out_features=768, bias=True)\n     (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n     (fc_out): Linear(in_features=768, out_features=64, bias=False)\n     (activation): GELU(approximate='none')\n   )\n   (mlp_local): MLPNetwork(\n     (fc1): Linear(in_features=768, out_features=768, bias=True)\n     (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n     (fc2): Linear(in_features=768, out_features=768, bias=True)\n     (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n     (fc_out): Linear(in_features=768, out_features=64, bias=False)\n     (activation): GELU(approximate='none')\n   )\n ),\n HT_HVAE_GenerativeNetwork(\n   (prior_mlp): MLPNetworkForPrior(\n     (fc1): Linear(in_features=64, out_features=128, bias=True)\n     (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n     (fc2): Linear(in_features=128, out_features=128, bias=True)\n     (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n     (fc_out): Linear(in_features=128, out_features=64, bias=False)\n     (activation): GELU(approximate='none')\n   )\n   (gru_initial_projection): Linear(in_features=32, out_features=768, bias=True)\n   (gru): GRU(32, 768, batch_first=True)\n   (gpt2_model): GPT2Model(\n     (wte): Embedding(50262, 768)\n     (wpe): Embedding(1024, 768)\n     (drop): Dropout(p=0.1, inplace=False)\n     (h): ModuleList(\n       (0-11): 12 x GPT2Block(\n         (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n         (attn): GPT2Attention(\n           (c_attn): Conv1D(nf=2304, nx=768)\n           (c_proj): Conv1D(nf=768, nx=768)\n           (attn_dropout): Dropout(p=0.1, inplace=False)\n           (resid_dropout): Dropout(p=0.1, inplace=False)\n         )\n         (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n         (mlp): GPT2MLP(\n           (c_fc): Conv1D(nf=3072, nx=768)\n           (c_proj): Conv1D(nf=768, nx=3072)\n           (act): NewGELUActivation()\n           (dropout): Dropout(p=0.1, inplace=False)\n         )\n       )\n     )\n     (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n   )\n   (global_projector): Linear(in_features=32, out_features=768, bias=True)\n   (latent_projection): Linear(in_features=768, out_features=768, bias=True)\n   (word_embedding): Embedding(50262, 768)\n   (final_linear): Linear(in_features=1536, out_features=50262, bias=False)\n ))"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "prio_net = generative_net.prior_mlp"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-09T03:51:27.413929Z",
          "iopub.execute_input": "2025-12-09T03:51:27.414529Z",
          "iopub.status.idle": "2025-12-09T03:51:27.418008Z",
          "shell.execute_reply.started": "2025-12-09T03:51:27.414505Z",
          "shell.execute_reply": "2025-12-09T03:51:27.417293Z"
        },
        "id": "KCcycmcZS3pO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Setup ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Instantiate Models\n",
        "hvae_transform = AutoregressiveHVAE_Transform(prio_net).to(device) # Pre-trained/Fixed backbone\n",
        "ebm = HierarchicalTransformerEBM().to(device)           # The model we are training\n",
        "\n",
        "optimizer = torch.optim.Adam(ebm.parameters(), lr=1e-4)\n",
        "\n",
        "# --- Diffusion Noise Schedule ---\n",
        "# Simple linear schedule for alpha/sigma\n",
        "T_steps = 100\n",
        "betas = torch.linspace(1e-4, 0.02, T_steps).to(device)\n",
        "alphas = 1.0 - betas\n",
        "alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
        "sigmas = torch.sqrt(1.0 - alphas_cumprod) # Approx noise level\n",
        "\n",
        "# --- Training Loop ---\n",
        "# Assume data_loader provides z_posterior samples from your trained Encoder\n",
        "# real_z_list = [z1_batch, z2_batch]\n",
        "# Assume optimizer, ebm, hvae_transform are defined\n",
        "# Assume standard scheduler vars (alphas, betas, alphas_cumprod) are defined\n",
        "\n",
        "for epoch in range(10):\n",
        "    for batch in new_train_loader: # Changed to 'batch' (dict) to access masks\n",
        "\n",
        "        # 0. Prepare Data & Masks\n",
        "        # Extract components from batch\n",
        "        # Note: We need 'enc_wordMask' to determine which sentences are Zombies\n",
        "        enc_word_mask = batch['enc_wordMask'].to(device) # [B, Max_Sent, Max_Words]\n",
        "\n",
        "        # Create Sentence Mask: 1.0 = Real, 0.0 = Zombie\n",
        "        # Logic: If first word is padded (0), the whole sentence is padded\n",
        "        sentence_mask = enc_word_mask[:, :, 0].float() # [B, Max_Sent]\n",
        "\n",
        "        # Expand for broadcasting against u_local [B, Max_Sent, 1]\n",
        "        local_valid_mask = sentence_mask.unsqueeze(-1)\n",
        "\n",
        "        # Get Latents\n",
        "        real_z_list = [\n",
        "            batch['global_latents'].to(device),\n",
        "            batch['local_latents'].to(device)\n",
        "        ]\n",
        "\n",
        "        # Get Context for EBM\n",
        "        context = batch['question_encoded'].to(device)\n",
        "        context_mask = batch['question_mask'].to(device)\n",
        "\n",
        "        batch_size = real_z_list[0].shape[0]\n",
        "\n",
        "        # 1. Inverse Transform: Get Real u_0\n",
        "        with torch.no_grad():\n",
        "            u0_list = hvae_transform.z_to_u(real_z_list)\n",
        "\n",
        "        # 2. Sample Random Time Step t\n",
        "        t_indices = torch.randint(0, T_steps, (batch_size,), device=device)\n",
        "\n",
        "        # Get params (Simplified for brevity)\n",
        "        a_t = torch.sqrt(alphas_cumprod[t_indices]).view(-1, 1)\n",
        "        s_t = torch.sqrt(1 - alphas_cumprod[t_indices]).view(-1, 1)\n",
        "\n",
        "        # Noise for Forward Process\n",
        "        # We generally don't need to mask this noise (u_t for zombies can be noisy),\n",
        "        # BUT technically u_0 for zombies is garbage anyway.\n",
        "        noise_list = [torch.randn_like(u) for u in u0_list]\n",
        "\n",
        "        # Create Noisy Target (u_t)\n",
        "        u_t_list = []\n",
        "        for i, (u0, eps) in enumerate(zip(u0_list, noise_list)):\n",
        "            # Broadcast helper\n",
        "            if u0.ndim == 3:\n",
        "                a_view, s_view = a_t.unsqueeze(1), s_t.unsqueeze(1)\n",
        "            else:\n",
        "                a_view, s_view = a_t, s_t\n",
        "\n",
        "            u_t_val = a_view * u0 + s_view * eps\n",
        "\n",
        "            # OPTIONAL: Force u_t zombies to be pure noise?\n",
        "            # Not strictly necessary if we mask updates, but cleaner.\n",
        "            u_t_list.append(u_t_val)\n",
        "\n",
        "        # Create Anchor (u_{t+1})\n",
        "        step_alpha = torch.sqrt(alphas[t_indices]).view(-1, 1)\n",
        "        step_sigma = torch.sqrt(betas[t_indices]).view(-1, 1)\n",
        "\n",
        "        u_anchor_list = []\n",
        "        for i, u_t in enumerate(u_t_list):\n",
        "            eps = torch.randn_like(u_t)\n",
        "            if u_t.ndim == 3:\n",
        "                sa_view, ss_view = step_alpha.unsqueeze(1), step_sigma.unsqueeze(1)\n",
        "            else:\n",
        "                sa_view, ss_view = step_alpha, step_sigma\n",
        "\n",
        "            u_anchor = sa_view * u_t + ss_view * eps\n",
        "            u_anchor_list.append(u_anchor)\n",
        "\n",
        "        # 3. Negative Sampling (Langevin Dynamics)\n",
        "        u_neg_list = [u.clone().detach().requires_grad_(True) for u in u_anchor_list]\n",
        "\n",
        "        # MCMC Loop\n",
        "        for k in range(10): # k steps\n",
        "            # A. Transform u -> z\n",
        "            z_neg_list = hvae_transform.u_to_z(u_neg_list)\n",
        "\n",
        "            # B. Compute Energy\n",
        "            t_input = t_indices.float().view(-1, 1)\n",
        "\n",
        "            # Pass Masks to EBM! (Using the logic we defined in CrossAttentionEBM)\n",
        "            energy = ebm(\n",
        "                z_neg_list,\n",
        "                context,\n",
        "                t_input,\n",
        "                context_mask=context_mask,\n",
        "                latent_word_mask=enc_word_mask # EBM uses this to mask pooling/attention\n",
        "            )\n",
        "\n",
        "            # C. Gradients\n",
        "            grads = torch.autograd.grad(energy, u_neg_list)\n",
        "\n",
        "            # D. Update with Masking Logic\n",
        "            new_u_list = []\n",
        "            for i, (u, g, anchor) in enumerate(zip(u_neg_list, grads, u_anchor_list)):\n",
        "\n",
        "                # Check dimensions to set views\n",
        "                if u.ndim == 3:\n",
        "                    ss_view = step_sigma.unsqueeze(1)\n",
        "                else:\n",
        "                    ss_view = step_sigma\n",
        "\n",
        "                # Calculate Forces\n",
        "                grad_anchor = -(u - anchor) / (ss_view**2 + 1e-6)\n",
        "                grad_prior = -u\n",
        "                total_grad = g + grad_anchor + grad_prior\n",
        "\n",
        "                # Step Size\n",
        "                step_size = 1e-2 * (ss_view**2)\n",
        "\n",
        "                # Noise\n",
        "                noise = torch.randn_like(u)\n",
        "\n",
        "                # --- CRITICAL: MASKING LOGIC ---\n",
        "                # Check if this is the Local Latent tensor (Index 1)\n",
        "                if i == 1:\n",
        "                    # 1. Mask the Gradient (Forces become 0 for Zombies)\n",
        "                    total_grad = total_grad * local_valid_mask\n",
        "\n",
        "                    # 2. Mask the Noise (No Brownian drift for Zombies)\n",
        "                    noise = noise * local_valid_mask\n",
        "                # -------------------------------\n",
        "\n",
        "                # Langevin Update\n",
        "                u_new = u + step_size * total_grad + torch.sqrt(2*step_size) * noise\n",
        "                new_u_list.append(u_new.detach().requires_grad_(True))\n",
        "\n",
        "            u_neg_list = new_u_list\n",
        "\n",
        "        # 4. Compute Loss\n",
        "        # Positive Energy\n",
        "        u_t_list = [u.detach().requires_grad_(True) for u in u_t_list]\n",
        "        z_pos_list = hvae_transform.u_to_z(u_t_list)\n",
        "        pos_energy = ebm(z_pos_list, context, t_input, context_mask, enc_word_mask)\n",
        "\n",
        "        # Negative Energy\n",
        "        z_neg_list = hvae_transform.u_to_z(u_neg_list)\n",
        "        neg_energy = ebm(z_neg_list, context, t_input, context_mask, enc_word_mask)\n",
        "\n",
        "        # Loss\n",
        "        loss = -(pos_energy - neg_energy)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Optional: Clip Gradients\n",
        "        torch.nn.utils.clip_grad_norm_(ebm.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        print(f\"Loss: {loss.item()}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-09T03:55:44.158779Z",
          "iopub.execute_input": "2025-12-09T03:55:44.159308Z",
          "iopub.status.idle": "2025-12-09T03:55:44.254821Z",
          "shell.execute_reply.started": "2025-12-09T03:55:44.159284Z",
          "shell.execute_reply": "2025-12-09T03:55:44.253905Z"
        },
        "id": "EqFSQmNBS3pO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# The generate_samples function outlines the reverse diffusion process for\n",
        "# sampling new latent variables (u) from the EBM. It initializes pure noise\n",
        "# and then iteratively refines the samples using Langevin dynamics based on the EBM's energy function.\n",
        "import torch\n",
        "\n",
        "def generate_samples(\n",
        "    hvae,           # The HVAE_Transform module\n",
        "    ebm,            # The HierarchicalEBM module\n",
        "    batch_size,\n",
        "    device,\n",
        "    n_langevin_steps=30, # Steps per diffusion level (Refinement)\n",
        "    step_size_base=1e-2  # Base learning rate for Langevin\n",
        "):\n",
        "    # --- 1. Define Noise Schedule (Must match training) ---\n",
        "    T = 100\n",
        "    betas = torch.linspace(1e-4, 0.02, T).to(device)\n",
        "    # In reverse, we need sigma_{t+1} for the anchor term\n",
        "    # For simplicity in this script, we approximate sigma ~ sqrt(beta)\n",
        "\n",
        "    # --- 2. Initialize u_T (Pure Noise) ---\n",
        "    # u1: Global [B, 128]\n",
        "    # u2: Sequence [B, 10, 128]\n",
        "    u1 = torch.randn(batch_size, 128, device=device)\n",
        "    u2 = torch.randn(batch_size, 10, 128, device=device)\n",
        "    u_list = [u1, u2]\n",
        "\n",
        "    print(\"Starting Reverse Diffusion...\")\n",
        "\n",
        "    # --- 3. Reverse Diffusion Loop (T-1 -> 0) ---\n",
        "    for t in range(T - 1, -1, -1):\n",
        "\n",
        "        # A. Define the Anchor (u_{t+1})\n",
        "        # The current state is u_{t+1}. We want to find u_t.\n",
        "        u_anchor_list = [u.clone().detach() for u in u_list]\n",
        "\n",
        "        # Get noise level for this step (sigma_{t+1})\n",
        "        # Used for the \"Leash\" strength\n",
        "        curr_beta = betas[t] # Scalar\n",
        "\n",
        "        # B. Inner Langevin Refinement Loop\n",
        "        for k in range(n_langevin_steps):\n",
        "            # Enable gradients\n",
        "            for u in u_list: u.requires_grad_(True)\n",
        "\n",
        "            # 1. Transform u -> z (Uses HVAE Decoder)\n",
        "            z_list = hvae.u_to_z(u_list)\n",
        "\n",
        "            # 2. Compute Energy\n",
        "            # Pass t as float tensor [B, 1]\n",
        "            t_tensor = torch.full((batch_size, 1), t, device=device).float()\n",
        "            energy = ebm(z_list, t_tensor)\n",
        "\n",
        "            # 3. Compute Gradients\n",
        "            grads = torch.autograd.grad(energy, u_list)\n",
        "\n",
        "            # 4. Update u\n",
        "            new_u_list = []\n",
        "            for u, g, anchor in zip(u_list, grads, u_anchor_list):\n",
        "                # Handle broadcasting\n",
        "                beta_view = curr_beta if u.ndim == 2 else curr_beta.view(1, 1, 1)\n",
        "\n",
        "                # Gradients\n",
        "                term_energy = g\n",
        "                term_prior = -u\n",
        "                # Anchor: -(u - anchor) / sigma^2\n",
        "                term_anchor = -(u - anchor) / (beta_view + 1e-6)\n",
        "\n",
        "                total_grad = term_energy + term_prior + term_anchor\n",
        "\n",
        "                # Scale step size by noise level (Heuristic from paper/Song et al.)\n",
        "                # Steps should be smaller when noise is small\n",
        "                s = step_size_base * beta_view\n",
        "\n",
        "                # Langevin Update\n",
        "                noise = torch.randn_like(u)\n",
        "                u_new = u + s * total_grad + torch.sqrt(2 * s) * noise\n",
        "\n",
        "                new_u_list.append(u_new.detach())\n",
        "\n",
        "            u_list = new_u_list\n",
        "\n",
        "        if t % 10 == 0:\n",
        "            print(f\"Step {t} finished.\")\n",
        "\n",
        "    # --- 4. Final Transform u_0 -> z_0 ---\n",
        "    # These are your high-quality latent codes ready for the decoder\n",
        "    final_z_list = hvae.u_to_z(u_list)\n",
        "\n",
        "    return final_z_list"
      ],
      "metadata": {
        "trusted": true,
        "id": "njTqKAv6S3pP"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}